{"cells":[{"cell_type":"code","source":["pip install keras"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"428b5f95-bb3c-41c6-9433-9b7922cfa0b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting keras\n  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\nInstalling collected packages: keras\nSuccessfully installed keras-2.8.0\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting keras\n  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\nInstalling collected packages: keras\nSuccessfully installed keras-2.8.0\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["pip install tensorflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50345e64-c678-43d1-af28-e7ec89f0027e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting tensorflow\n  Using cached tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl (497.6 MB)\nCollecting libclang&gt;=9.0.1\n  Using cached libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\nRequirement already satisfied: keras&lt;2.9,&gt;=2.8.0rc0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-69417e07-6fd7-40a2-9d81-6f85d1154bcb/lib/python3.8/site-packages (from tensorflow) (2.8.0)\nCollecting wrapt&gt;=1.11.0\n  Using cached wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (52.0.0)\nCollecting flatbuffers&gt;=1.12\n  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\nCollecting astunparse&gt;=1.6.0\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting tensorflow-io-gcs-filesystem&gt;=0.23.1\n  Using cached tensorflow_io_gcs_filesystem-0.25.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\nCollecting tf-estimator-nightly==2.8.0.dev2021122109\n  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\nCollecting opt-einsum&gt;=2.3.2\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nRequirement already satisfied: protobuf&gt;=3.9.2 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (3.17.2)\nCollecting google-pasta&gt;=0.1.1\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting absl-py&gt;=0.4.0\n  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\nCollecting termcolor&gt;=1.1.0\n  Using cached termcolor-1.1.0-py3-none-any.whl\nCollecting gast&gt;=0.2.1\n  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\nRequirement already satisfied: six&gt;=1.12.0 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\nCollecting tensorboard&lt;2.9,&gt;=2.8\n  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\nCollecting keras-preprocessing&gt;=1.1.1\n  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\nCollecting numpy&gt;=1.20\n  Using cached numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\nCollecting typing-extensions&gt;=3.6.6\n  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)\nCollecting grpcio&lt;2.0,&gt;=1.24.3\n  Using cached grpcio-1.46.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\nCollecting h5py&gt;=2.9.0\n  Using cached h5py-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /databricks/python3/lib/python3.8/site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.36.2)\nCollecting werkzeug&gt;=0.11.15\n  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /databricks/python3/lib/python3.8/site-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow) (2.25.1)\nCollecting google-auth&lt;3,&gt;=1.6.3\n  Using cached google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\nCollecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1\n  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nCollecting tensorboard-data-server&lt;0.7.0,&gt;=0.6.0\n  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\nCollecting tensorboard-plugin-wit&gt;=1.6.0\n  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\nCollecting markdown&gt;=2.6.8\n  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\nCollecting pyasn1-modules&gt;=0.2.1\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\nCollecting rsa&lt;5,&gt;=3.1.4\n  Using cached rsa-4.8-py3-none-any.whl (39 kB)\nCollecting cachetools&lt;6.0,&gt;=2.0.0\n  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\nCollecting requests-oauthlib&gt;=0.7.0\n  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nCollecting importlib-metadata&gt;=4.4\n  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\nCollecting zipp&gt;=0.5\n  Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB)\nCollecting pyasn1&lt;0.5.0,&gt;=0.4.6\n  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow) (4.0.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow) (2.10)\nCollecting oauthlib&gt;=3.0.0\n  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\nInstalling collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.19.2\n    Not uninstalling numpy at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-69417e07-6fd7-40a2-9d81-6f85d1154bcb\n    Can&#39;t uninstall &#39;numpy&#39;. No files were found to uninstall.\nSuccessfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.0 h5py-3.6.0 importlib-metadata-4.11.3 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 numpy-1.22.3 oauthlib-3.2.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 werkzeug-2.1.2 wrapt-1.14.1 zipp-3.8.0\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting tensorflow\n  Using cached tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl (497.6 MB)\nCollecting libclang&gt;=9.0.1\n  Using cached libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\nRequirement already satisfied: keras&lt;2.9,&gt;=2.8.0rc0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-69417e07-6fd7-40a2-9d81-6f85d1154bcb/lib/python3.8/site-packages (from tensorflow) (2.8.0)\nCollecting wrapt&gt;=1.11.0\n  Using cached wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (52.0.0)\nCollecting flatbuffers&gt;=1.12\n  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\nCollecting astunparse&gt;=1.6.0\n  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting tensorflow-io-gcs-filesystem&gt;=0.23.1\n  Using cached tensorflow_io_gcs_filesystem-0.25.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\nCollecting tf-estimator-nightly==2.8.0.dev2021122109\n  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\nCollecting opt-einsum&gt;=2.3.2\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nRequirement already satisfied: protobuf&gt;=3.9.2 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (3.17.2)\nCollecting google-pasta&gt;=0.1.1\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting absl-py&gt;=0.4.0\n  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\nCollecting termcolor&gt;=1.1.0\n  Using cached termcolor-1.1.0-py3-none-any.whl\nCollecting gast&gt;=0.2.1\n  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\nRequirement already satisfied: six&gt;=1.12.0 in /databricks/python3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\nCollecting tensorboard&lt;2.9,&gt;=2.8\n  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\nCollecting keras-preprocessing&gt;=1.1.1\n  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\nCollecting numpy&gt;=1.20\n  Using cached numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\nCollecting typing-extensions&gt;=3.6.6\n  Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB)\nCollecting grpcio&lt;2.0,&gt;=1.24.3\n  Using cached grpcio-1.46.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\nCollecting h5py&gt;=2.9.0\n  Using cached h5py-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /databricks/python3/lib/python3.8/site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.36.2)\nCollecting werkzeug&gt;=0.11.15\n  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /databricks/python3/lib/python3.8/site-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow) (2.25.1)\nCollecting google-auth&lt;3,&gt;=1.6.3\n  Using cached google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\nCollecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1\n  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nCollecting tensorboard-data-server&lt;0.7.0,&gt;=0.6.0\n  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\nCollecting tensorboard-plugin-wit&gt;=1.6.0\n  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\nCollecting markdown&gt;=2.6.8\n  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\nCollecting pyasn1-modules&gt;=0.2.1\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\nCollecting rsa&lt;5,&gt;=3.1.4\n  Using cached rsa-4.8-py3-none-any.whl (39 kB)\nCollecting cachetools&lt;6.0,&gt;=2.0.0\n  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\nCollecting requests-oauthlib&gt;=0.7.0\n  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\nCollecting importlib-metadata&gt;=4.4\n  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\nCollecting zipp&gt;=0.5\n  Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB)\nCollecting pyasn1&lt;0.5.0,&gt;=0.4.6\n  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow) (4.0.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow) (2.10)\nCollecting oauthlib&gt;=3.0.0\n  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\nInstalling collected packages: pyasn1, zipp, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.19.2\n    Not uninstalling numpy at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-69417e07-6fd7-40a2-9d81-6f85d1154bcb\n    Can&#39;t uninstall &#39;numpy&#39;. No files were found to uninstall.\nSuccessfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.0 h5py-3.6.0 importlib-metadata-4.11.3 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 numpy-1.22.3 oauthlib-3.2.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 werkzeug-2.1.2 wrapt-1.14.1 zipp-3.8.0\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nimport pickle\nimport argparse\nimport os, sys\n# import tensorflow as tf\nfrom os.path import join\n# import wordseq_models\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.layers import Embedding\nfrom keras.utils.vis_utils import plot_model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ed1c85d-146f-4d81-85f8-941a741276f5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# def parse_args():\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument('--datafile',  dest='datafile', help='input pickle file', default='./data/DATA_WORDSEQV0_HADM_TOP10.p', type = str)\n# #     parser.add_argument('--embmatrix', dest='embmatrix', help='embedding matrix', default='./data/EMBMATRIXV0_WORD2VEC_v2_300dim.p', type = str)\n# #     parser.add_argument('--epoch',      dest='nb_epoch', help='number of epoch', default=50, type = int)\n# #     parser.add_argument('--batch_size', dest='batch_size', help='batch size', default=128, type = int)\n# #     parser.add_argument('--model_name', dest='model_name', help='model loaded from *_model.py', default='conv1d_1', type=str)\n# #     parser.add_argument('--append_name', dest='append_name', help='load weights_model_name<append_name>', default='', type=str)\n# #     parser.add_argument('--pre_train', dest = 'pre_train', help='continue train from pretrained para? True/False', default=False)\n# #     parser.add_argument('--gpu', dest = 'gpu', help='set gpu no to be used (default: 0)', default='1',type=str)\n# #     parser.add_argument('--plot_model', dest = 'plot_model', help='plot the said model', default=True)\n# #     parser.add_argument('--patience', dest ='patience', help='patient for early stopper', default=5, type=int)\n#     parser.add_argument('--labelmode', dest ='labelmode', \n#                         help='additional label processing. Option: tile<num>, repeat<num>, range<num>_<num>',\n#                         default='', type=str)\n\n# #     if len(sys.argv) == 1:\n# #         parser.print_help()\n# #         print('Use Default Settings ......')\n\n#     args = parser.parse_args()\n#     return args"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5662d859-a41b-45c2-bc8b-3abf94f03706"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from keras.models import *\nfrom keras.layers import *\n\ndef lstm_1(input_shape, output_shape, embedding_layer):\n    print('Build model...')\n    model = Sequential()\n    model.add(embedding_layer)\n    model.add(LSTM(256, return_sequences=True))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(LSTM(64))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    model.add(Dense(output_shape, activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc', 'mse'])\n    model.summary()\n    return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80160cbe-097c-456b-8d4b-23f9353c504f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["f = open(\"/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10.p\", 'rb')\nembmatrix = \"/dbfs/FileStore/data/EMBMATRIXV1_WORD2VEC_v2_300dim.p\"\nloaded_data = []\nfor i in range(1): # [reverse_dictionary, train_sequence, test_sequence, train_label, test_label]:\n    loaded_data.append(pickle.load(f))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34587624-acd5-47dc-a400-4d4dd9543332"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dffe0637-923d-41eb-a9ad-e1ce2fa56e79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-399288262320924&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>pickle<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_data.p&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: file must have &#39;read&#39; and &#39;readline&#39; attributes</div>","errorSummary":"<span class=\"ansi-red-fg\">TypeError</span>: file must have &#39;read&#39; and &#39;readline&#39; attributes","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-399288262320924&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>pickle<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_data.p&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: file must have &#39;read&#39; and &#39;readline&#39; attributes</div>"]}}],"execution_count":0},{"cell_type":"code","source":["f = open(\"/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_data.p\")\nfor i in range(1):\n  loaded_train_data = pickle.load(f)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79ae1d6d-8940-4b46-96b0-343442ad3f61"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">UnicodeDecodeError</span>                        Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-399288262320923&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> f <span class=\"ansi-blue-fg\">=</span> open<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_data.p&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-green-fg\">for</span> i <span class=\"ansi-green-fg\">in</span> range<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\">   </span>loaded_train_data <span class=\"ansi-blue-fg\">=</span> pickle<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/codecs.py</span> in <span class=\"ansi-cyan-fg\">decode</span><span class=\"ansi-blue-fg\">(self, input, final)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    320</span>         <span class=\"ansi-red-fg\"># decode input (taking the buffer into account)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    321</span>         data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>buffer <span class=\"ansi-blue-fg\">+</span> input\n<span class=\"ansi-green-fg\">--&gt; 322</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-blue-fg\">(</span>result<span class=\"ansi-blue-fg\">,</span> consumed<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_buffer_decode<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>errors<span class=\"ansi-blue-fg\">,</span> final<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    323</span>         <span class=\"ansi-red-fg\"># keep undecoded input until the next call</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>         self<span class=\"ansi-blue-fg\">.</span>buffer <span class=\"ansi-blue-fg\">=</span> data<span class=\"ansi-blue-fg\">[</span>consumed<span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-red-fg\">UnicodeDecodeError</span>: &#39;utf-8&#39; codec can&#39;t decode byte 0x80 in position 0: invalid start byte</div>","errorSummary":"<span class=\"ansi-red-fg\">UnicodeDecodeError</span>: &#39;utf-8&#39; codec can&#39;t decode byte 0x80 in position 0: invalid start byte","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">UnicodeDecodeError</span>                        Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-399288262320923&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> f <span class=\"ansi-blue-fg\">=</span> open<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_data.p&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-green-fg\">for</span> i <span class=\"ansi-green-fg\">in</span> range<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\">   </span>loaded_train_data <span class=\"ansi-blue-fg\">=</span> pickle<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/codecs.py</span> in <span class=\"ansi-cyan-fg\">decode</span><span class=\"ansi-blue-fg\">(self, input, final)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    320</span>         <span class=\"ansi-red-fg\"># decode input (taking the buffer into account)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    321</span>         data <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>buffer <span class=\"ansi-blue-fg\">+</span> input\n<span class=\"ansi-green-fg\">--&gt; 322</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-blue-fg\">(</span>result<span class=\"ansi-blue-fg\">,</span> consumed<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_buffer_decode<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>errors<span class=\"ansi-blue-fg\">,</span> final<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    323</span>         <span class=\"ansi-red-fg\"># keep undecoded input until the next call</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>         self<span class=\"ansi-blue-fg\">.</span>buffer <span class=\"ansi-blue-fg\">=</span> data<span class=\"ansi-blue-fg\">[</span>consumed<span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-red-fg\">UnicodeDecodeError</span>: &#39;utf-8&#39; codec can&#39;t decode byte 0x80 in position 0: invalid start byte</div>"]}}],"execution_count":0},{"cell_type":"code","source":["f.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0c66bfc-847a-4e68-ad9c-44f232a1e198"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["reverse_dictionary = loaded_data[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27c189b6-83d2-4edc-a9c1-6d16183f4aac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def train():\n    nb_epoch = 10\n    batch_size = 128\n    model_name = lstm_1\n    pre_train = False\n    labelmode = \"tile\"\n    f = open(\"/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10.p\", 'rb')\n    embmatrix = \"/dbfs/FileStore/data/EMBMATRIXV1_WORD2VEC_v2_300dim.p\"\n    loaded_data = []\n#     for i in range(7): # [reverse_dictionary, train_sequence, test_sequence, train_label, test_label]:\n#         loaded_data.append(pickle.load(f))\n#     f.close()\n\n    dictionary = reverse_dictionary\n    train_sequence = pickle.load(\"/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_data.p\")\n    val_sequence = pickle.load(\"/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_valid_data.p\") \n    train_label = pickle.load(\"/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_labels.p\")  \n    val_label = pickle.load(\"/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_valid_labels.p\")\n\n    if labelmode[:4] == 'tile':\n        n = int(labelmode[4:].strip())\n        train_label = np.tile(train_label, n)\n        val_label = np.tile(val_label, n)\n        print('labelmode: tile {0}'.format(train_label.shape))\n#     elif args.labelmode[:6] == 'repeat':\n#         n = int(args.labelmode[6:].strip())\n#         train_label = np.repeat(train_label, n, axis=1)\n#         val_label = np.repeat(val_label, n, axis=1)\n#         print('labelmode: repeat {0}'.format(train_label.shape))\n#     elif args.labelmode[:5] == 'range':\n#         n = [int(i) for i in args.labelmode[5:].split(\"_\")]\n#         train_label = train_label[:,n[0]:n[1]]\n#         val_label = val_label[:,n[0]:n[1]]\n#         print('labelmode: range{0}'.format(train_label.shape))\n\n    # f = open('./data/dictionary_v0.p', 'wb')\n    # cPickle.dump(dictionary, f)\n    # f.close()\n\n    f = open(embmatrix)\n    embedding_matrix = pickle.load(f)\n    f.close()\n\n    max_sequence_length = train_sequence.shape[1]\n    vocabulary_size = len(dictionary) + 1\n    embedding_dim = embedding_matrix.shape[1]\n    category_number = train_label.shape[1]\n    input_shape = train_sequence.shape[1:]\n\n    embedding_layer = Embedding(vocabulary_size,\n                        embedding_dim,\n                        weights=[embedding_matrix],\n                        input_length=max_sequence_length,\n                        trainable=False,\n                        input_shape=input_shape)\n\n    model_func = lstm_1\n    model = model_func(input_shape, category_number, embedding_layer)\n\n#     if args.plot_model:\n#         fig_name = './data/cache/' + args.model_name + '.png'\n#         plot_model(model, fig_name, True, False)\n        # return\n\n#     if not os.path.isdir('./data/cache'):\n#         os.mkdir('./data/cache')\n    weight_name = 'weights_' + model_name + args.append_name + '.h5'\n    weights_path = join('/dbfs/FileStore/data/cache', weight_name)\n#     if pre_train:\n#         model.load_weights(weights_path)\n\n    print('checkpoint')\n    checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1, save_best_only=True)\n    earlystopping = EarlyStopping(monitor='val_loss', patience=args.patience, verbose=0, mode='auto')\n    print('early stop at ', args.patience)\n\n    #train_sequence = np.concatenate((train_sequence, val_sequence), axis=0)\n    #train_label = np.concatenate((train_label, val_label), axis=0)\n\n    model.fit(train_sequence, train_label,\n              batch_size = batch_size,\n              epochs = nb_epoch,\n              validation_data = [val_sequence, val_label],\n              callbacks=[checkpointer, earlystopping])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06cf8437-8c6d-4dc8-ae3a-8155db2d89b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# args = parse_args()\ntrain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2853a82c-f06b-4d9f-82c5-f29eedfb965d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-499586464024917&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># args = parse_args()</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>train<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-499586464024916&gt;</span> in <span class=\"ansi-cyan-fg\">train</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     13</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span>     dictionary <span class=\"ansi-blue-fg\">=</span> reverse_dictionary\n<span class=\"ansi-green-fg\">---&gt; 15</span><span class=\"ansi-red-fg\">     </span>train_sequence <span class=\"ansi-blue-fg\">=</span> pickle<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_data.p&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     16</span>     val_sequence <span class=\"ansi-blue-fg\">=</span> pickle<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_valid_data.p&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     17</span>     train_label <span class=\"ansi-blue-fg\">=</span> pickle<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_labels.p&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: file must have &#39;read&#39; and &#39;readline&#39; attributes</div>","errorSummary":"<span class=\"ansi-red-fg\">TypeError</span>: file must have &#39;read&#39; and &#39;readline&#39; attributes","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-499586464024917&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># args = parse_args()</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>train<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-499586464024916&gt;</span> in <span class=\"ansi-cyan-fg\">train</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     13</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span>     dictionary <span class=\"ansi-blue-fg\">=</span> reverse_dictionary\n<span class=\"ansi-green-fg\">---&gt; 15</span><span class=\"ansi-red-fg\">     </span>train_sequence <span class=\"ansi-blue-fg\">=</span> pickle<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_data.p&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     16</span>     val_sequence <span class=\"ansi-blue-fg\">=</span> pickle<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_valid_data.p&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     17</span>     train_label <span class=\"ansi-blue-fg\">=</span> pickle<span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/FileStore/data/DATA_WORDSEQV0_HADM_TOP10_train_labels.p&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: file must have &#39;read&#39; and &#39;readline&#39; attributes</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e7c6e07-aff4-43fd-bb4c-fc861f97085f"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"wordseq_train","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":499586464024911}},"nbformat":4,"nbformat_minor":0}
